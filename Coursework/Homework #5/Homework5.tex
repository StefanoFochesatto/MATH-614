
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% This top part of the document is called the 'preamble'.  Modify it with caution!
%
% The real document starts below where it says 'The main document starts here'.

\documentclass[12pt]{article}

\usepackage{amssymb,amsmath,amsthm}
\usepackage[top=1in, bottom=1in, left=1.25in, right=1.25in]{geometry}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}
% Comment the following line to use TeX's default font of Computer Modern.
\usepackage{times,txfonts}
\usepackage{mwe}
\usepackage{caption}
\usepackage{subcaption}




\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newtheoremstyle{homework}% name of the style to be used
  {18pt}% measure of space to leave above the theorem. E.g.: 3pt
  {12pt}% measure of space to leave below the theorem. E.g.: 3pt
  {}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\bfseries}% name of head font
  {:}% punctuation between head and body
  {2ex}% space after theorem head; " " = normal interword space
  {}% Manually specify head
\theoremstyle{homework} 

% Set up an Exercise environment and a Solution label.
\newtheorem*{exercisecore}{Exercise \@currentlabel}
\newenvironment{exercise}[1]
{\def\@currentlabel{#1}\exercisecore}
{\endexercisecore}

\newcommand{\localhead}[1]{\par\smallskip\noindent\textbf{#1}\nobreak\\}%
\newcommand\solution{\localhead{Solution:}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Stuff for getting the name/document date/title across the header
\makeatletter
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\fancyfoot[C]{\ifnum \value{page} > 1\relax\thepage\fi}
\fancyhead[L]{\ifx\@doclabel\@empty\else\@doclabel\fi}
\fancyhead[C]{\ifx\@docdate\@empty\else\@docdate\fi}
\fancyhead[R]{\ifx\@docauthor\@empty\else\@docauthor\fi}
\headheight 15pt

\def\doclabel#1{\gdef\@doclabel{#1}}
\doclabel{Use {\tt\textbackslash doclabel\{MY LABEL\}}.}
\def\docdate#1{\gdef\@docdate{#1}}
\docdate{Use {\tt\textbackslash docdate\{MY DATE\}}.}
\def\docauthor#1{\gdef\@docauthor{#1}}
\docauthor{Use {\tt\textbackslash docauthor\{MY NAME\}}.}
\makeatother

% Shortcuts for blackboard bold number sets (reals, integers, etc.)
\newcommand{\Reals}{\ensuremath{\mathbb R}}
\newcommand{\Nats}{\ensuremath{\mathbb N}}
\newcommand{\Ints}{\ensuremath{\mathbb Z}}
\newcommand{\Rats}{\ensuremath{\mathbb Q}}
\newcommand{\Cplx}{\ensuremath{\mathbb C}}
%% Some equivalents that some people may prefer.
\let\RR\Reals
\let\NN\Nats
\let\II\Ints
\let\CC\Cplx

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% The main document start here.

% The following commands set up the material that appears in the header.
\doclabel{Math 614: Homework 5}
\docauthor{Stefano Fochesatto}
\docdate{\today}


\begin{document}


\begin{exercise}{P12}
  \begin{enumerate}
    \item[a.] Give an example of a projector which is not an orthogonal projector.\\
    \solution Consider the outer-product of any two, non orthogonal vectors. This will create a projector
    whose complimentary subspace is not orthogonal to it's own subspace.
    Consider the following outer-product, 
    \begin{equation*}
      P = 
      \begin{bmatrix}
        1\\
        1
      \end{bmatrix}
      \begin{bmatrix}
        1 & 0
      \end{bmatrix}
      =
      \begin{bmatrix}
        1 & 0\\
        1 & 0
      \end{bmatrix}
    \end{equation*}

    Clearly this is not an orthogonal projector since,
    \begin{equation*}
      P =       
      \begin{bmatrix}
        1 & 0\\
        1 & 0
      \end{bmatrix}
      \neq 
      \begin{bmatrix}
        1 & 1\\
        0 & 0
      \end{bmatrix}
      = P^*z
    \end{equation*}
    We could also show that they are not orthogonal by computing the complimentary projector, $I - P$
    passing in some vector $v$ to both $P$ and $I - P$ and show that the resulting vectors are not orthogonal via 
    inner product. 
    \vspace{.15in}
    
    \item[b.] Show that if $P$ is a projector and $\lambda$ is an eigenvalue of $P$ then 
    $\lambda = 0$ or $\lambda = 1$. \\
    \solution Suppose $P$ is a projector and consider the eigenvector equation for some $\lambda$ and $x$,
    \begin{equation*}
      Px = \lambda x
    \end{equation*}
    Recall that by definition a projector has the following property, $P = P^2$ by substitution we get,
    \begin{align*}
      P^2x &= \lambda x,\\
      PPx &= \lambda x,\\
      P(Px) &= \lambda x.
    \end{align*}
    Substituting the eigenvector identity we get, 
      \begin{align*}
      P(\lambda x) &= \lambda x,\\
      \lambda Px &= \lambda x,\\
      \lambda \lambda x &= \lambda x,\\
      \lambda^2x - \lambda x &= 0,\\
      (\lambda^2 - \lambda)x &= 0,\\
      ((\lambda)(\lambda - 1))x &= 0.
    \end{align*}
    Solving the equation, it must be the case that $\lambda = 0$ or $\lambda = 1$.
    \vspace{.15in}
    
    \item[c.] Show that if a projector is invertible then it is the identity.\\
    \solution Suppose a projector $P$ has an inverse such that $PP^{-1} = I$. By definition
    since $P$ is a projector the following is true,
    \begin{equation*}
      P^2x = Px.
    \end{equation*}
    Applying $P^{-1}$ to both sides we get, 
    \begin{align*}
      P^{-1}PPx =  P^{-1}Px,\\
      Px =  Ix,\\
      Px =  x.
    \end{align*}
    Thus $P$ must be the matrix multiplicative identity, so $P = I$.
    \vspace{.15in}
  \end{enumerate}
\end{exercise}
\vspace{1in}




\begin{exercise}{P13} Just for fun write a Matlab/Octave function\\
  function P = randproj(m,k)
  which generates a random orthogonal projection $P \in \RR^{mxm}$ with $rank(P) = k$. Note that 
  $k$ is an integer in the rank $0\le k \le n$. Verify the function by checking the output is an orthogonal projector 
  and that it has the desired rank. \\
  \solution Recall from chapter six, that for an orthogonal projector $P$ there exists some orthonormal basis $Q$ on $\RR^{mxm}$, such that,
  \begin{equation*}
    P = \hat{Q}\hat{Q}^*
  \end{equation*}
  \textbf{Code:}
  \begin{center}
    \lstinputlisting[basicstyle = \small]{randProj.jl}
    \end{center} 
  \textbf{Console:}
  \begin{center}
    \lstinputlisting[basicstyle = \small]{randprojconsole.txt}
    \end{center} 
    As expected we have a projector since when we computed the 
    norm of $P^2 - P$ we essential got zero minding rounding error. Similarly when we 
    check the $SVD$ of our projector we get 3 non-zero singular values therefore we have 
    a rank-3 projector as expected. 
\end{exercise}
\vspace{1in}


\begin{exercise}{5.4} Suppose $A \in \CC^{mxm}$ has an $SVD$ $A = U\Sigma V^*$. Find the eigenvalue decomposition of the 
  $2mx2m$ hermitian matrix,
  \begin{equation*}
    \begin{bmatrix}
      0 & A^*\\
      A & 0
    \end{bmatrix}.
  \end{equation*}
  \solution Since $A = U\Sigma V^*$ we know that, 
  \begin{equation*}
    AV = U\Sigma.
  \end{equation*}
  Concequently we know that $A^* = V \Sigma U^*$, and therefore,
  \begin{equation*}
    A^*U = V\Sigma. 
  \end{equation*}
  Written as a system of block matrices with respect to $B$ we get, 
  \begin{equation*}
    \begin{bmatrix}
      0 & A\\
      A^* & 0
    \end{bmatrix}
    \begin{bmatrix}
      U\\
      V
    \end{bmatrix}
    \begin{bmatrix}
      U\Sigma\\
      V\Sigma
    \end{bmatrix}
  \end{equation*}
  Similarly the following is true, 
  \begin{equation*}
    \begin{bmatrix}
      0 & A\\
      A^* & 0
    \end{bmatrix}
    \begin{bmatrix}
      -U\\
      V
    \end{bmatrix}
     = 
    \begin{bmatrix}
      U\Sigma\\
      -V\Sigma
    \end{bmatrix}.
  \end{equation*}
  Through block matrix multiplication  and factoring it follows that, 
  \begin{equation*}
    \begin{bmatrix}
      0 & A\\
      A^* & 0
    \end{bmatrix}
    \begin{bmatrix}
      -U & U\\
      V & V
    \end{bmatrix}
    =
   \begin{bmatrix}
      U\Sigma & U\Sigma\\
      -V\Sigma & V\Sigma
    \end{bmatrix}
    = 
    \begin{bmatrix}
      U & U\\
      -V & V
    \end{bmatrix}
    \begin{bmatrix}
      \Sigma & 0\\
      0 & \Sigma
    \end{bmatrix}
  \end{equation*}
  We want an expression of the form, $AX = X\Lambda$ so we can solve for $A$ by right multiplying the expression 
  by $X^{-1}$ and attain the eigenvalue decomposition. Note that by block matrix multiplication the following is also equivalent, 
  \begin{equation*}
    \begin{bmatrix}
      0 & A\\
      A^* & 0
    \end{bmatrix}
    \begin{bmatrix}
      -U & U\\
      V & V
    \end{bmatrix}
     = 
     \begin{bmatrix}
      -U & U\\
      V & V
    \end{bmatrix}
    \begin{bmatrix}
      -\Sigma & 0\\
      0 & \Sigma
    \end{bmatrix}
  \end{equation*}
  Therefore we can left multiply by $X^{-1}$ to get our eigenvalue decomposition, 
  \begin{equation*}
    \begin{bmatrix}
      0 & A\\
      A^* & 0
    \end{bmatrix}
     = 
     \begin{bmatrix}
      -U & U\\
      V & V
    \end{bmatrix}
    \begin{bmatrix}
      -\Sigma & 0\\
      0 & \Sigma
    \end{bmatrix}
    \begin{bmatrix}
      -U & U\\
      V & V
    \end{bmatrix}^{-1}.
  \end{equation*}




\end{exercise}
\vspace{1in}

\begin{exercise}{6.4} Consider the matrices, 
  \begin{equation*}
    A = 
    \begin{bmatrix}
      1 & 0\\
      0 & 1\\
      1 & 0
    \end{bmatrix},
    B = 
    \begin{bmatrix}
      1 & 2\\
      0 & 1\\
      1 & 0
    \end{bmatrix}
  \end{equation*}
  Answer the following questions by hand calculation.\\
  \begin{enumerate}
    \item[a.] What is the orthogonal projector $P$ onto $range(A)$, and what is the image under $P$ of the 
    vector $[1 2 3]'$? \\
    \solution Given the simplicity of $A$ we can quickly compute an orthonormal basis on the range of $A$. Note that the columns are already orthogonal, all we have to do 
    is normalize the first column. Doing so we get, 
    \begin{equation*}
      Q = 
      \begin{bmatrix}
        \frac{1}{\sqrt{2}} & 0\\
        0 & 1\\
        \frac{1}{\sqrt{2}} & 0
      \end{bmatrix}
    \end{equation*}
    Applying $P = QQ^*$ we get,
    \begin{equation*}
      P = 
      \begin{bmatrix}
        \frac{1}{\sqrt{2}} & 0\\
        0 & 1\\
        \frac{1}{\sqrt{2}} & 0
      \end{bmatrix}
      \begin{bmatrix}
        \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}\\
        0 & 1& 0
        \end{bmatrix}
        =
        \begin{bmatrix}
          \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}\\
          0 & 1 & 0\\
          \frac{1}{\sqrt{2}} & 0& \frac{1}{\sqrt{2}}
        \end{bmatrix}
    \end{equation*} 
    Computing $P[1 2 3]'$ to find the image of $[1 2 3]'$ under $P$, 
    \begin{equation*}
      \begin{bmatrix}
        \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}\\
        0 & 1 & 0\\
        \frac{1}{\sqrt{2}} & 0& \frac{1}{\sqrt{2}}
      \end{bmatrix}
      \begin{bmatrix}
        1\\
        2\\
        3
      \end{bmatrix} = 
      \begin{bmatrix}
        2\\
        2\\
        2
      \end{bmatrix}.
    \end{equation*}
    \vspace{.15in}


    \item[b.] To get an an orthogonal basis on the range of $B$ by hand, we can use Gram-Schmidt, or alternatively use the formula, 
    \begin{equation*}
      P = B(B^*B)^{-1}B^*
    \end{equation*} 
    Since there are only two vectors, I will proceed with Gram-Schmidt. Let $b_i$ be the columns if $B$, and $q_i$ form the orthonormal basis on the 
    range of $B$,
    \begin{equation*}
      q_1 = \frac{b_1}{||b_1||} = 
      \begin{bmatrix}
        \frac{1}{\sqrt{2}}\\
        0\\
        \frac{1}{\sqrt{2}}
      \end{bmatrix}. 
    \end{equation*}
    For the second vector we do the following, 
    \begin{align*}
    v_2 &= b_2 - q_1^*b_2q_1,\\
      &=
      \begin{bmatrix}
        2\\
        1\\
        0
      \end{bmatrix}
      -
      \sqrt{2}
      \begin{bmatrix}
        \frac{1}{\sqrt{2}}\\
        0\\
        \frac{1}{\sqrt{2}}
      \end{bmatrix},\\
      &=
      \begin{bmatrix}
        2\\
        1\\
        0
      \end{bmatrix}
      - 
      \begin{bmatrix}
        1\\
        0\\
        1
      \end{bmatrix},\\
      &=
      \begin{bmatrix}
        1\\
        1\\
        -1
      \end{bmatrix}.
    \end{align*}
    \begin{equation*}
      q_2 = \frac{v_2}{||v_2||} = \frac{v_2}{\sqrt{3}}
      \begin{bmatrix}
       \frac{\sqrt{3}}{3}\\
       \frac{\sqrt{3}}{3}\\
       -\frac{\sqrt{3}}{3}
      \end{bmatrix}.
    \end{equation*}
    Putting our column vectors together we get our orthonormal basis $Q$, 
    
    \begin{equation*}
      Q = 
      \begin{bmatrix}
        \frac{\sqrt{2}}{2} & \frac{\sqrt{3}}{3}\\
        0 & \frac{\sqrt{3}}{3}\\
        \frac{\sqrt{2}}{2} & -\frac{\sqrt{3}}{3}
      \end{bmatrix}
    \end{equation*}
    Computing $P$ with $P = QQ^*$ we get, 
    \begin{equation*}
      P = \begin{bmatrix}\frac{5}{6}&\frac{1}{3}&\frac{1}{6}\\ \frac{1}{3}&\frac{1}{3}&-\frac{1}{3}\\ \frac{1}{6}&-\frac{1}{3}&\frac{5}{6}\end{bmatrix}.
    \end{equation*}
    Computing $P[1 2 3]'$ to find the image of $[1 2 3]'$ under $P$, 
    \begin{equation*}
      \begin{bmatrix}\frac{5}{6}&\frac{1}{3}&\frac{1}{6}\\ \frac{1}{3}&\frac{1}{3}&-\frac{1}{3}\\ \frac{1}{6}&-\frac{1}{3}&\frac{5}{6}\end{bmatrix}.
      \begin{bmatrix}
        1\\
        2\\
        3
      \end{bmatrix} = 
      \begin{bmatrix}2\\ 0\\ 2\end{bmatrix}
    \end{equation*}
  \end{enumerate}
\end{exercise}
\vspace{1in}


\begin{exercise}{7.1} Consider again the matrixes $A$ and $B$ of exercise 6.4 (the previous exercise)\\
  \begin{enumerate}
    \item[a.] Using any method you like, determine a reduced and full QR factorization for the matrix $A$.\\
    \solution Recall from the previous problem our orthonormal basis on the range of $A$,
    \begin{equation*}
      \hat{Q} =  \begin{bmatrix}
        \frac{1}{\sqrt{2}} & 0\\
        0 & 1\\
        \frac{1}{\sqrt{2}} & 0
      \end{bmatrix}
    \end{equation*}
    Note that this matrix was produced by normalizing the first column of $A$, undoing that operation we get the following $R$,
    \begin{equation*}
      \hat{R} = \begin{bmatrix}
        \sqrt{2} & 0\\
        0 & 1
      \end{bmatrix}.
    \end{equation*}
    In order to get a full QR factorization we need an orthonormal basis for the full $\CC^{3x3}$ space. Geometrically we can see that the 
    following is a suitable basis, 
    \begin{equation*}
      Q = 
      \begin{bmatrix}
        \frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}}\\
        0 & 1 & 0\\
        \frac{1}{\sqrt{2}} & 0 * \frac{1}{\sqrt{2}}
      \end{bmatrix}.
    \end{equation*}
    The full $R$ matrix simply adds another row of zeroes, so we get
    \begin{equation*}
      \hat{R} = \begin{bmatrix}
        \sqrt{2} & 0\\
        0 & 1\\
        0 & 0
      \end{bmatrix}.
    \end{equation*}
    \vspace{.15in}
    \item[b.] Recall that in the previous problem we used Gram-Schmidt to find the following, orthonormal basis on the range of $B$,  
    \begin{equation*}
      \hat{Q} = 
      \begin{bmatrix}
        \frac{\sqrt{2}}{2} & \frac{\sqrt{3}}{3}\\
        0 & \frac{\sqrt{3}}{3}\\
        \frac{\sqrt{2}}{2} & -\frac{\sqrt{3}}{3}
      \end{bmatrix}.
    \end{equation*} 
    Recalling the Gram-Schmidt process from the previous problem we can also form our matrix $\hat[R]$ by solving for the $b_i$ in each equation,
    \begin{align*}
      q_1 &= \frac{b_1}{||b_1||},\\
      b_1 &= ||b_1||q_1,\\
      b_1 &= \sqrt{2}q_1.
    \end{align*}
    \begin{align*}
      q_2 &= \dfrac{b_2 - q_1^*b_2q_1}{||v_2||},\\
      b_2&= (q_1^*b_2)q_1 + ||v_2||q_2,\\
      b_2&= (\sqrt{2})q_1 + (\sqrt{3})q_2.
    \end{align*}
    Forming the matrix $\hat{R}$ wih our coefficients, we get
    \begin{equation*}
      \hat{R} = 
      \begin{bmatrix}
        \sqrt{2} & \sqrt{2}\\
        0 & \sqrt{3}
      \end{bmatrix}.
    \end{equation*}
    In order to get a full QR factorization we need an orthonormal basis for the full $\CC^{3x3}$ space. To do so lets consider a third step in the Gram-Schmidt process 
    on the vector $b_3 = [1 0 0]'$,
    \begin{align*}
      v_3 &= b_3 - (q_1^*b_3)q_1 - (q_2^*b_3)q_2,\\
      &= \begin{bmatrix}
        1\\
        0\\
        0
      \end{bmatrix}
      -\frac{\sqrt{2}}{2}
      \begin{bmatrix}
        \frac{\sqrt{2}}{2}\\
        0\\
        \frac{\sqrt{2}}{2}
      \end{bmatrix}
      -
      \frac{\sqrt{3}}{3}
      \begin{bmatrix}
        \frac{\sqrt{3}}{3}\\
        \frac{\sqrt{3}}{3}\\
        -\frac{\sqrt{3}}{3}
      \end{bmatrix},\\
      & = \begin{bmatrix}\frac{1}{6}\\ -\frac{1}{3}\\ -\frac{1}{6}\end{bmatrix}.
    \end{align*}
    \begin{equation*}
      q_3 = \frac{v_3}{||v_3||} = \frac{v_3}{1/\sqrt{6}} = \begin{bmatrix}\frac{\sqrt{6}}{6}\\ -\frac{\sqrt{6}}{3}\\ -\frac{\sqrt{6}}{6}\end{bmatrix}.
    \end{equation*}
    Therefore our full QR factorization comes out to, 
    \begin{equation*}
      Q = \begin{bmatrix}
        \frac{\sqrt{2}}{2} & \frac{\sqrt{3}}{3} &\frac{\sqrt{6}}{6} \\
        0 & \frac{\sqrt{3}}{3}& -\frac{\sqrt{6}}{3}\\
        \frac{\sqrt{2}}{2} & -\frac{\sqrt{3}}{3} & -\frac{\sqrt{6}}{6}
      \end{bmatrix}.
    \end{equation*}
    \begin{equation*}
      R = 
      \begin{bmatrix}
        \sqrt{2} & \sqrt{2}\\
        0 & \sqrt{3}\\
        0 & 0
      \end{bmatrix}.
    \end{equation*}
  \end{enumerate}
\end{exercise}
\vspace{1in}

\begin{exercise}{7.3} Let $A$ be an $mxm$ matrix, and let $a_j$ be its $j$th column. Give an algebraic proof of Hadamard's inequality: 
  \begin{equation*}
    |det(A)|\leq \prod_{j = 1}^m ||a_j||_2.
  \end{equation*}
  Also give a geometric interpretation of this result, making use of the fact tha t the determinant equals the volume of a parallelepiped.\\
  \solution Suppose $A$ is an $mxm$ invertible matrix (otherwise the result is trivial). Consider the $A = QR$ factorization, column-wise we know that, 
  \begin{equation*}
    a_j = \sum_{k = 1}^m r_{kj}q_k,
  \end{equation*}
  where $r_{kj}$ are the entries of $R$ and $q_k$ are the columns of $Q$. Taking the norm of both sides, 
  \begin{equation*}
    ||a_j||_2 = ||\sum_{k = 1}^m r_{kj}q_k||_2 = \sum_{k = 1}^m |r_{kj}|||q_k||_2. 
  \end{equation*}
  Note that the $||q_k||_2 = 1$, therefore we get the following, 
  \begin{equation*}
    ||a_j||_2 = \sum_{k = 1}^m |r_{kj}|.
  \end{equation*}
  Considering only the diagonal entries(subtracting non-diagonal terms) in the sum it must follow that,
  \begin{equation*}
    ||a_j||_2 \geq |r_{jj}|.
  \end{equation*}
  Now consider the following, 
  \begin{equation*}
    det(A) = det(QR) = |det(Q)||det(R)| = 1|det(R)| = \prod_{j = 1}^m|r_{jj}| \leq \prod_{j = 1}^m ||a_j||_2.
  \end{equation*}
  The fact that $det(Q) = 1$ follows from, 
  \begin{align*}
    QQ^* &= I,\\
    det(QQ^*) &= 1,\\
    det(Q)det(Q^*) &= 1,\\
    det(Q)det(Q)^* &= 1,\\
    |det(Q)| &= 1.
  \end{align*}
  The substitution of $|det(R)| = \prod_{j = 1}^m|r_{jj}|$ follows from performing co-factor expansion on a triangular matrix. The final 
  inequality comes from substitution. \\
  Geometrically, the absolute value of the determinant of $A$ is the volume of the parallelepiped where each length is a column vector of $A$. The 
  product in the Hadamard inequality is the volume of the box where each length is a column vector of $A$.
  Consider the equation for the volume of the $A$ is $3x3$ parallelepiped(scalar triple product),
  \begin{equation*}
    Volume = |det(A)| = |a_1||a_2||a_3|cos(\phi)
  \end{equation*} 
  where $\phi$ is the angle between $a_3$ and vector perpendicular both $a_1$ and $a_2$. Under the constraints of having the lengths 
  be column vectors of $A$, a box will have the maximum volume since it minimizes $\phi$. 
\end{exercise}
\vspace{1in}

\begin{exercise}{8.1} Let $A$ be an $mxn$ matrix. Determine the exact numbers of floating point additions, subtractions, multiplcations, and divisions
  involved in computing the factorization of $A = \hat{Q}\hat{R}$ in MGS.\\
  \solution Let's start by first considering the MGS algorithm, \\
  \textbf{MGS Algorithm:}
  \begin{center}
    \lstinputlisting[basicstyle = \small]{MGS.txt}
    \end{center}
  Let's start by counting the total number of additions in the algorithm. Consider the line $r_{ii} = ||v_i||$ computing the norm of a length $m$
  vector requires $m-1$ additions. In the second loop consider the line $r_{ij} = q^*_iv_j$, computing an inner-product on length $m$ vectors requires 
  $m-1$ additions. Taking into account the nesting of the loops we get the following 
  \begin{equation*}
    Additions = \sum_{i = 1}^n m-1 + \sum_{i = 1}^n\sum_{j = i + 1}^n m-1
  \end{equation*}
  To count the total number of multiplications, let's start by consider the line $r_{ii} = ||v_i||$, computing a norm of a length $m$ vector requires \
  $m$ multiplications. In the second loop consider the line, $r_{ij} = q^*_iv_j$, computing an inner-product on length $m$ vectors requires 
  $m$ multiplications. Th second loop also has the line $v_j = v_j -r_{ij}q_i$, applying a scalar to a length $m$ vector also requires $m$ multiplications. 
  Taking into account the nesting of the loops we get the following 
  \begin{equation*}
    Multiplications = \sum_{i = 1}^n m + \sum_{i = 1}^n\sum_{j = i + 1}^n 2m
  \end{equation*}
  Counting the number of subtractions, we can look at line $v_j = v_j -r_{ij}q_i$. Subtracting two length $m$ vectors, requires $m$ subtractions. Given that 
  this line is in a nested for loop we get the following, 
  \begin{equation*}
    Subtractions = \sum_{i = 1}^n\sum_{j = i + 1}^n m
  \end{equation*} 
  Counting divisions, there is only one line $q_i = v_i/r_ii$. Normalizing a length $m$ vector, requires $m$ divisions. Since the line is in a for loop we get the following, 
  \begin{equation*}
    Divisions = \sum_{i = 1}^n m
  \end{equation*}
  \end{exercise}
  \vspace{1in}

  \begin{exercise}{8.2} Write a MATALB funciton $[Q, R] = MGS(A)$ that computes a reduced $QR$ factorizaiton $A = \hat{Q}\hat{R}$ of an $mxn$ matrix $A$
    with $m\geq n$ using modified Gram-Schmidt orthogonalization.\\
    \solution \\
    \textbf{Code:}
    \begin{center}
      \lstinputlisting[basicstyle = \small]{mgs.jl}
      \end{center}
  \end{exercise}




\end{document}


































